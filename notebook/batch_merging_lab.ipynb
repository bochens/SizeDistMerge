{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2690e8a3",
   "metadata": {},
   "source": [
    "## Current ARCSIX Campaign Size Distribution Merge R1 Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c87c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset  # kept because your pipeline imports it elsewhere\n",
    "\n",
    "BASE_DIR = Path.cwd().parent\n",
    "sys.path.append(str(BASE_DIR / \"src\"))\n",
    "\n",
    "from merge_production import (  # noqa: E402\n",
    "    load_aufi_oneday_v2,\n",
    "    split_frames,\n",
    "    plot_period_totals,\n",
    "    make_filtered_specs_v2,\n",
    "    run_joint_optimization_v2,\n",
    "    plot_history,\n",
    "    make_consensus_merged_spec_v2,\n",
    "    plot_sizedist_all,\n",
    "    write_day_netcdf_v2,\n",
    "    chunk_is_incloud,\n",
    "    filter_chunk_by_inlet_flag,\n",
    ")\n",
    "from sizedist_utils import remap_dndlog_by_edges_any  # noqa: E402\n",
    "from ict_utils import read_inlet_flag, read_microphysical  # noqa: E402\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SETTINGS\n",
    "# =============================================================================\n",
    "dates = [\n",
    "    \"2024-05-28\", \"2024-05-30\", \"2024-05-31\", \"2024-06-03\",\n",
    "    \"2024-06-05\", \"2024-06-06\", \"2024-06-07\", \"2024-06-10\",\n",
    "    \"2024-06-11\", \"2024-06-13\", \"2024-07-25\", \"2024-07-29\",\n",
    "    \"2024-07-30\", \"2024-08-01\", \"2024-08-02\", \"2024-08-07\",\n",
    "    \"2024-08-08\", \"2024-08-09\", \"2024-08-15\",\n",
    "]\n",
    "\n",
    "DATA_DIR  = Path(\"/Volumes/Hailstone Data/Research Data/ARCSIX_P3B\")\n",
    "aps_dir   = DATA_DIR / \"LARGE-APS\"\n",
    "uhsas_dir = DATA_DIR / \"PUTLS-UHSAS\"\n",
    "fims_dir  = DATA_DIR / \"FIMS\"\n",
    "pops_dir  = DATA_DIR / \"PUTLS-POPS\"\n",
    "inlet_dir = DATA_DIR / \"LARGE-InletFlag\"\n",
    "micro_dir = DATA_DIR / \"LARGE-MICROPHYSICAL\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"/Users/C832577250/Output/arcsix_sizedist_merge_batch_v3\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_FILE   = OUTPUT_DIR / \"output_log.txt\"\n",
    "ERROR_LOG  = OUTPUT_DIR / \"error_log.txt\"\n",
    "\n",
    "with LOG_FILE.open(\"a\") as f:\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    f.write(\"ARCSIX Aerosol Size Distribution Merge Log - VERSION 3 (4-Instrument)\\n\")\n",
    "    f.write(f\"Generated: {ts} by Bo Chen\\n\\n\")\n",
    "    f.write(f\"DATA_DIR:      {DATA_DIR}\\n\")\n",
    "    f.write(f\"OUTPUT_DIR:    {OUTPUT_DIR}\\n\")\n",
    "    f.write(f\"DATES:         {dates}\\n\\n\")\n",
    "\n",
    "MERGE_PER = 5                 # minutes\n",
    "FIMS_LAG  = 10                # seconds (shift FIMS earlier)\n",
    "INCLOUD_PAD_S = 10            # seconds around inlet_flag to mark a chunk in-cloud\n",
    "MIN_SAMPLES_PER_INST = 50\n",
    "\n",
    "# --- NEW: overlap requirement ---\n",
    "MIN_OVERLAP_S = 120           # minimum overlap among all instruments (seconds)\n",
    "OVERLAP_FREQ  = \"1S\"          # compute overlap at 1-second resolution\n",
    "\n",
    "# ---------- OPTIMIZATION SETTINGS ----------------------------------------------------\n",
    "MOMENT = \"V\"\n",
    "SPACE  = \"linear\"\n",
    "PAIR_W = 1.0\n",
    "\n",
    "BOUNDS_UHSAS = [(1.3, 1.8)]\n",
    "BOUNDS_APS   = [(950.0, 2000.0)]\n",
    "\n",
    "UHSAS_XMIN = None\n",
    "UHSAS_XMAX = None\n",
    "\n",
    "FIMS_XMIN = 10\n",
    "FIMS_XMAX = 500\n",
    "\n",
    "POPS_XMIN = None\n",
    "POPS_XMAX = None\n",
    "\n",
    "LUT_DIR   = BASE_DIR / \"lut\"\n",
    "\n",
    "# ---------------- GLOBAL BINNING (IDENTICAL ACROSS ALL DAYS) ------------------------\n",
    "# 100 bins => 101 edges\n",
    "FINE_BIN = 100\n",
    "\n",
    "GLOBAL_XMIN_NM = 10.0\n",
    "GLOBAL_XMAX_NM = 5000.0\n",
    "\n",
    "GLOBAL_EDGES = np.logspace(\n",
    "    np.log10(GLOBAL_XMIN_NM),\n",
    "    np.log10(GLOBAL_XMAX_NM),\n",
    "    FINE_BIN + 1\n",
    ").astype(float)\n",
    "\n",
    "# ---------- CONSENSUS SETTINGS ---------------------------------------------------\n",
    "ALPHA_FIMS  = 1.0\n",
    "ALPHA_UHSAS = 1.5\n",
    "ALPHA_POPS  = 0.2\n",
    "ALPHA_APS   = 2.0\n",
    "LAMBDA_TIK  = 1e-5\n",
    "C_PUNISH    = 0.5\n",
    "COMBINATION_SPACE = \"log10\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OVERLAP QC\n",
    "# =============================================================================\n",
    "def overlap_seconds_all_instruments(\n",
    "    chunk: dict[str, pd.DataFrame],\n",
    "    t_start: pd.Timestamp,\n",
    "    t_end: pd.Timestamp,\n",
    "    instruments: tuple[str, ...] = (\"APS\", \"UHSAS\", \"FIMS\", \"POPS\"),\n",
    "    freq: str = \"1S\",\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Count how many seconds in [t_start, t_end) have >=1 record from ALL instruments.\n",
    "\n",
    "    This is strict: same-second presence for all instruments.\n",
    "    \"\"\"\n",
    "    if t_start is None or t_end is None:\n",
    "        return 0\n",
    "    if t_end <= t_start:\n",
    "        return 0\n",
    "\n",
    "    # Fixed second grid, end-exclusive\n",
    "    grid = pd.date_range(t_start, t_end, freq=freq, inclusive=\"left\")\n",
    "    if len(grid) == 0:\n",
    "        return 0\n",
    "\n",
    "    all_mask = np.ones(len(grid), dtype=bool)\n",
    "\n",
    "    for name in instruments:\n",
    "        df = chunk.get(name, None)\n",
    "        if df is None or df.empty:\n",
    "            return 0\n",
    "\n",
    "        dfi = df.loc[t_start:t_end]\n",
    "        if dfi.empty:\n",
    "            return 0\n",
    "\n",
    "        pres = (dfi.resample(freq).size() > 0)\n",
    "        pres = pres.reindex(grid, fill_value=False)\n",
    "\n",
    "        all_mask &= pres.to_numpy()\n",
    "        if not all_mask.any():\n",
    "            return 0\n",
    "\n",
    "    return int(all_mask.sum())\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN LOOP\n",
    "# =============================================================================\n",
    "for a_date in dates:\n",
    "    with LOG_FILE.open(\"a\") as f:\n",
    "        f.write(\"----------------------------------------------------------\\n\")\n",
    "        f.write(f\"Merging {a_date} every {MERGE_PER} minutes\\n\")\n",
    "        f.write(\"------------------------ SETTINGS ------------------------\\n\")\n",
    "        # --- Time & Sampling ---\n",
    "        f.write(f\"FIMS_LAG: {FIMS_LAG}s  # shift FIMS earlier\\n\")\n",
    "        f.write(f\"INCLOUD_PAD_S: {INCLOUD_PAD_S}s  # cloud-flag buffer\\n\")\n",
    "        f.write(f\"MIN_SAMPLES_PER_INST: {MIN_SAMPLES_PER_INST}  # min avg count to process chunk\\n\")\n",
    "        f.write(f\"MERGE_PER: {MERGE_PER} min  # chunk duration\\n\")\n",
    "\n",
    "        # --- Overlap QC ---\n",
    "        f.write(f\"\\n# OVERLAP QC\\n\")\n",
    "        f.write(f\"MIN_OVERLAP_S: {MIN_OVERLAP_S}  # required overlap seconds among all instruments\\n\")\n",
    "        f.write(f\"OVERLAP_FREQ: {OVERLAP_FREQ}  # resampling frequency to compute overlap\\n\")\n",
    "\n",
    "        # --- Optimization Physics ---\n",
    "        f.write(f\"\\n# OPTIMIZATION (Joint Fit)\\n\")\n",
    "        f.write(f\"MOMENT: {MOMENT}  # (N=0, S=2, V=3)\\n\")\n",
    "        f.write(f\"SPACE: {SPACE}  # linear or log cost\\n\")\n",
    "        f.write(f\"PAIR_W: {PAIR_W}  # cross-instrument consistency weight\\n\")\n",
    "        f.write(f\"BOUNDS_UHSAS (m): {BOUNDS_UHSAS}  # real refractive index range\\n\")\n",
    "        f.write(f\"BOUNDS_APS (rho): {BOUNDS_APS}  # density range kg/m3\\n\")\n",
    "\n",
    "        # --- Instrument Range Filters ---\n",
    "        f.write(f\"\\n# INSTRUMENT X-LIMITS (nm)\\n\")\n",
    "        f.write(f\"UHSAS_RANGE: {UHSAS_XMIN} to {UHSAS_XMAX}\\n\")\n",
    "        f.write(f\"FIMS_RANGE:  {FIMS_XMIN} to {FIMS_XMAX}\\n\")\n",
    "        f.write(f\"POPS_RANGE:  {POPS_XMIN} to {POPS_XMAX}\\n\")\n",
    "\n",
    "        # --- Global Output Bins ---\n",
    "        f.write(f\"\\n# GLOBAL OUTPUT BINS (IDENTICAL ACROSS ALL DAYS)\\n\")\n",
    "        f.write(f\"FINE_BIN: {FINE_BIN}\\n\")\n",
    "        f.write(f\"GLOBAL_XMIN_NM: {GLOBAL_XMIN_NM}\\n\")\n",
    "        f.write(f\"GLOBAL_XMAX_NM: {GLOBAL_XMAX_NM}\\n\")\n",
    "\n",
    "        # --- Consensus & Tikhonov ---\n",
    "        f.write(f\"\\n# MERGING & CONSENSUS\\n\")\n",
    "        f.write(f\"LAMBDA_TIK: {LAMBDA_TIK}  # Tikhonov smoothing parameter\\n\")\n",
    "        f.write(f\"C_PUNISH: {C_PUNISH}  # consensus penalty parameter\\n\")\n",
    "        f.write(f\"COMBINATION_SPACE: {COMBINATION_SPACE}  # linear or log combination\\n\")\n",
    "        f.write(f\"ALPHA_FIMS:  {ALPHA_FIMS}  # weight for FIMS in consensus\\n\")\n",
    "        f.write(f\"ALPHA_UHSAS: {ALPHA_UHSAS}  # weight for UHSAS in consensus\\n\")\n",
    "        f.write(f\"ALPHA_POPS:  {ALPHA_POPS}  # weight for POPS in consensus\\n\")\n",
    "        f.write(f\"ALPHA_APS:   {ALPHA_APS}  # weight for APS in consensus\\n\")\n",
    "        f.write(\"----------------------------------------------------------\\n\\n\")\n",
    "\n",
    "    day_dir = OUTPUT_DIR / a_date\n",
    "    day_dir.mkdir(parents=True, exist_ok=True)\n",
    "    totals_dir = day_dir / \"time_series\"\n",
    "    opt_dir    = day_dir / \"loss_curve\"\n",
    "    plots_dir  = day_dir / \"merge_plots\"\n",
    "    totals_dir.mkdir(exist_ok=True)\n",
    "    opt_dir.mkdir(exist_ok=True)\n",
    "    plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # IMPORTANT: use global edges for ALL days (not per-day)\n",
    "    common_edges = GLOBAL_EDGES\n",
    "\n",
    "    day_fims_algn    = []\n",
    "    day_uhsas_algn   = []\n",
    "    day_pops_algn    = []\n",
    "    day_aps_algn     = []\n",
    "    day_merged       = []\n",
    "    day_times_start  = []\n",
    "    day_times_end    = []\n",
    "    day_incloud      = []\n",
    "    orig_APS_edges   = None\n",
    "    orig_UHSAS_edges = None\n",
    "    orig_POPS_edges  = None\n",
    "    orig_FIMS_edges  = None\n",
    "    day_n_fit        = []\n",
    "    day_n_pops_fit   = []\n",
    "    day_rho_fit      = []\n",
    "    day_best_cost    = []\n",
    "\n",
    "    filtered_frames = load_aufi_oneday_v2(a_date, aps_dir, uhsas_dir, fims_dir, pops_dir)\n",
    "\n",
    "    # shift FIMS earlier by FIMS_LAG seconds\n",
    "    if \"FIMS\" in filtered_frames and not filtered_frames[\"FIMS\"].empty:\n",
    "        filtered_frames[\"FIMS\"] = filtered_frames[\"FIMS\"].copy()\n",
    "        filtered_frames[\"FIMS\"].index = (\n",
    "            filtered_frames[\"FIMS\"].index - pd.Timedelta(seconds=FIMS_LAG)\n",
    "        )\n",
    "\n",
    "    inlet_flag = read_inlet_flag(inlet_dir, start=a_date, end=None, prefix=\"ARCSIX\")\n",
    "    micro      = read_microphysical(micro_dir, start=a_date, end=None, prefix=\"ARCSIX\")\n",
    "    cpc_total  = pd.to_numeric(micro.get(\"CNgt10nm\"), errors=\"coerce\")\n",
    "\n",
    "    split_filtered_frames = split_frames(filtered_frames, MERGE_PER * 60)\n",
    "\n",
    "    for i, a_chunk in enumerate(split_filtered_frames):\n",
    "        try:\n",
    "            times   = [t for df in a_chunk.values() if len(df) for t in (df.index[0], df.index[-1])]\n",
    "            t_start = min(times) if times else None\n",
    "            t_end   = max(times) if times else None\n",
    "\n",
    "            with LOG_FILE.open(\"a\") as f:\n",
    "                f.write(f\"\\tsizedist {i:03d}: {t_start} -> {t_end}\\n\")\n",
    "\n",
    "            inlet_chunk     = inlet_flag.loc[t_start:t_end] if (t_start is not None and t_end is not None) else inlet_flag.iloc[0:0]\n",
    "            cpc_total_chunk = cpc_total.loc[t_start:t_end]  if (t_start is not None and t_end is not None) else cpc_total.iloc[0:0]\n",
    "\n",
    "            inc_flag = chunk_is_incloud(inlet_flag, t_start, t_end, tol_s=INCLOUD_PAD_S)\n",
    "\n",
    "            if t_start is None or t_end is None:\n",
    "                with LOG_FILE.open(\"a\") as f:\n",
    "                    f.write(f\"\\t[SKIP] chunk {i:03d} empty window\\n\")\n",
    "                continue\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # FILTER OUT inlet-flagged time (±INCLOUD_PAD_S seconds)\n",
    "            # ---------------------------------------------------\n",
    "            a_chunk = filter_chunk_by_inlet_flag(\n",
    "                chunk=a_chunk,\n",
    "                inlet_flag=inlet_flag,\n",
    "                t_start=t_start,\n",
    "                t_end=t_end,\n",
    "                pad_s=INCLOUD_PAD_S,\n",
    "            )\n",
    "\n",
    "            if all((df is None or df.empty) for df in a_chunk.values()):\n",
    "                with LOG_FILE.open(\"a\") as f:\n",
    "                    f.write(f\"\\t[SKIP] chunk {i:03d} all data removed by inlet_flag filter\\n\")\n",
    "                continue\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # NEW: enforce >=2 minutes overlap among ALL instruments\n",
    "            # ---------------------------------------------------\n",
    "            ov_s = overlap_seconds_all_instruments(\n",
    "                chunk=a_chunk,\n",
    "                t_start=t_start,\n",
    "                t_end=t_end,\n",
    "                instruments=(\"APS\", \"UHSAS\", \"FIMS\", \"POPS\"),\n",
    "                freq=OVERLAP_FREQ,\n",
    "            )\n",
    "            if ov_s < MIN_OVERLAP_S:\n",
    "                with LOG_FILE.open(\"a\") as f:\n",
    "                    f.write(\n",
    "                        f\"\\t[SKIP] chunk {i:03d} insufficient overlap: \"\n",
    "                        f\"{ov_s:d}s < {MIN_OVERLAP_S:d}s (freq={OVERLAP_FREQ})\\n\"\n",
    "                    )\n",
    "                continue\n",
    "\n",
    "            # 1) time series plot\n",
    "            fig1, _ = plot_period_totals(\n",
    "                a_chunk,\n",
    "                title=f\"{a_date} sizedist {i:03d}\",\n",
    "                inlet_flag=inlet_flag,\n",
    "                gauss_win=10,\n",
    "                gauss_std=2,\n",
    "                cpc_total=cpc_total_chunk,\n",
    "                t_start=t_start,\n",
    "                t_end=t_end,\n",
    "            )\n",
    "            fig1.savefig(totals_dir / f\"sizedist_{i:03d}_totals.png\", dpi=150)\n",
    "            plt.close(fig1)\n",
    "\n",
    "            # 2) mean specs\n",
    "            specs, line_kwargs, fill_kwargs, bin_counts = make_filtered_specs_v2(\n",
    "                a_chunk,\n",
    "                a_chunk.get(\"APS\",   pd.DataFrame()),\n",
    "                a_chunk.get(\"UHSAS\", pd.DataFrame()),\n",
    "                a_chunk.get(\"FIMS\",  pd.DataFrame()),\n",
    "                a_chunk.get(\"POPS\",  pd.DataFrame()),\n",
    "                LOG_FILE,\n",
    "            )\n",
    "\n",
    "            if (\"APS\" not in specs) or (\"UHSAS\" not in specs) or (\"FIMS\" not in specs) or (\"POPS\" not in specs):\n",
    "                with LOG_FILE.open(\"a\") as f:\n",
    "                    f.write(f\"\\t[SKIP] chunk {i:03d} missing instrument(s)\\n\")\n",
    "                continue\n",
    "\n",
    "            # gate on non-zero average counts per instrument (your existing logic)\n",
    "            low_data_reason = None\n",
    "            for _name in (\"APS\", \"UHSAS\", \"FIMS\", \"POPS\"):\n",
    "                if _name in bin_counts and len(bin_counts[_name]) > 0:\n",
    "                    arr = np.asarray(bin_counts[_name], int)\n",
    "                    nz = arr[arr > 0]\n",
    "                    nz_avg = float(nz.mean()) if nz.size > 0 else 0.0\n",
    "                    if nz_avg < MIN_SAMPLES_PER_INST:\n",
    "                        low_data_reason = f\"{_name} nonzero_avg={nz_avg:.1f} < {MIN_SAMPLES_PER_INST}\"\n",
    "                        break\n",
    "            if low_data_reason is not None:\n",
    "                with LOG_FILE.open(\"a\") as f:\n",
    "                    f.write(f\"\\t[SKIP] chunk {i:03d} low data: {low_data_reason}\\n\")\n",
    "                continue\n",
    "\n",
    "            # save original instrument edges once\n",
    "            if orig_APS_edges is None:\n",
    "                orig_APS_edges = specs[\"APS\"][1]\n",
    "            if orig_UHSAS_edges is None:\n",
    "                orig_UHSAS_edges = specs[\"UHSAS\"][1]\n",
    "            if orig_FIMS_edges is None:\n",
    "                orig_FIMS_edges = specs[\"FIMS\"][1]\n",
    "            if orig_POPS_edges is None:\n",
    "                orig_POPS_edges = specs[\"POPS\"][1]\n",
    "\n",
    "            # 3) optimization\n",
    "            specs_opt, line_kwargs_opt, fill_kwargs_opt, opt_res = run_joint_optimization_v2(\n",
    "                specs,\n",
    "                line_kwargs,\n",
    "                fill_kwargs,\n",
    "                moment=MOMENT,\n",
    "                space=SPACE,\n",
    "                pair_w=PAIR_W,\n",
    "                uhsas_bounds=BOUNDS_UHSAS,\n",
    "                aps_bounds=BOUNDS_APS,\n",
    "                uhsas_xmin=UHSAS_XMIN,\n",
    "                uhsas_xmax=UHSAS_XMAX,\n",
    "                fims_xmin=FIMS_XMIN,\n",
    "                fims_xmax=FIMS_XMAX,\n",
    "                pops_xmin=POPS_XMIN,\n",
    "                pops_xmax=POPS_XMAX,\n",
    "                lut_dir=LUT_DIR,\n",
    "            )\n",
    "\n",
    "            # 4) loss curve\n",
    "            fig_h, ax_h = plot_history(opt_res[\"hist\"])\n",
    "            ax_h.set_title(f\"opt hist {a_date} {i:03d}\")\n",
    "            fig_h.savefig(opt_dir / f\"sizedist_{i:03d}_opt_hist.png\", dpi=150)\n",
    "            plt.close(fig_h)\n",
    "\n",
    "            # 5) log\n",
    "            with LOG_FILE.open(\"a\") as f:\n",
    "                f.write(\n",
    "                    f\"\\t\\tUHSAS n_fit = {opt_res['n_fit']:.4f}, \"\n",
    "                    f\"POPS n_fit = {opt_res['n_pops_fit']:.4f}, \"\n",
    "                    f\"APS rho_fit = {opt_res['rho_fit']:.1f} kg/m^3, \"\n",
    "                    f\"cost = {opt_res['best_cost']:.6g}\\n\\n\"\n",
    "                )\n",
    "\n",
    "            # 6) Consensus merged (native edges; then remap to GLOBAL_EDGES below)\n",
    "            uh_label = f\"UHSAS fit (n={opt_res['n_fit']:.3f})\"\n",
    "            po_label = f\"POPS fit (n={opt_res['n_pops_fit']:.3f})\"\n",
    "            ap_label = f\"APS fit (ρ={opt_res['rho_fit']*0.001:.3f} g/cm$^3$)\"\n",
    "\n",
    "            tik_specs, tik_lines, tik_fills, tik_diag = make_consensus_merged_spec_v2(\n",
    "                e_fims_sel=specs_opt[\"FIMS_applied\"][1],\n",
    "                y_fims_sel=specs_opt[\"FIMS_applied\"][2],\n",
    "                e_uhsas_fit=specs_opt[uh_label][1],\n",
    "                y_uhsas_fit=specs_opt[uh_label][2],\n",
    "                e_pops_fit=specs_opt[po_label][1],\n",
    "                y_pops_fit=specs_opt[po_label][2],\n",
    "                e_aps_fit=specs_opt[ap_label][1],\n",
    "                y_aps_fit=specs_opt[ap_label][2],\n",
    "                lam=LAMBDA_TIK,\n",
    "                n_points=FINE_BIN,  # 100 bins native output of consensus step\n",
    "                alpha_fims=ALPHA_FIMS,\n",
    "                alpha_uhsas=ALPHA_UHSAS,\n",
    "                alpha_pops=ALPHA_POPS,\n",
    "                alpha_aps=ALPHA_APS,\n",
    "                c_punish=C_PUNISH,\n",
    "                data_space=COMBINATION_SPACE,\n",
    "            )\n",
    "\n",
    "            specs_opt.update(tik_specs)\n",
    "            line_kwargs_opt.update(tik_lines)\n",
    "            fill_kwargs_opt.update(tik_fills)\n",
    "\n",
    "            # 7) plots\n",
    "            (figN, axN), (figV, axV), _ = plot_sizedist_all(\n",
    "                specs=specs_opt,\n",
    "                merged_spec=tik_specs,\n",
    "                line_kwargs=line_kwargs_opt,\n",
    "                merged_line_kwargs=tik_lines,\n",
    "                fill_kwargs=fill_kwargs_opt,\n",
    "                merged_fill_kwargs=tik_fills,\n",
    "                inlet_flag=inlet_chunk,\n",
    "                d_str=a_date,\n",
    "            )\n",
    "            figN.savefig(plots_dir / f\"{a_date}_chunk{i:03d}_dNdlogDp.png\", dpi=200)\n",
    "            plt.close(figN)\n",
    "            figV.savefig(plots_dir / f\"{a_date}_chunk{i:03d}_dVdlogDp.png\", dpi=200)\n",
    "            plt.close(figV)\n",
    "\n",
    "            # 8) collect + rebin onto GLOBAL_EDGES (IDENTICAL ACROSS ALL DAYS)\n",
    "            tik_name  = next(iter(tik_specs.keys()))\n",
    "            tik_edges = tik_specs[tik_name][1]\n",
    "            tik_vals  = tik_specs[tik_name][2]\n",
    "\n",
    "            fims_on_common   = remap_dndlog_by_edges_any(\n",
    "                specs_opt[\"FIMS_applied\"][1], common_edges, specs_opt[\"FIMS_applied\"][2]\n",
    "            )\n",
    "            uhsas_on_common  = remap_dndlog_by_edges_any(\n",
    "                specs_opt[uh_label][1], common_edges, specs_opt[uh_label][2]\n",
    "            )\n",
    "            pops_on_common   = remap_dndlog_by_edges_any(\n",
    "                specs_opt[po_label][1], common_edges, specs_opt[po_label][2]\n",
    "            )\n",
    "            aps_on_common    = remap_dndlog_by_edges_any(\n",
    "                specs_opt[ap_label][1], common_edges, specs_opt[ap_label][2]\n",
    "            )\n",
    "            merged_on_common = remap_dndlog_by_edges_any(\n",
    "                tik_edges, common_edges, tik_vals\n",
    "            )\n",
    "\n",
    "            day_fims_algn.append(fims_on_common)\n",
    "            day_uhsas_algn.append(uhsas_on_common)\n",
    "            day_pops_algn.append(pops_on_common)\n",
    "            day_aps_algn.append(aps_on_common)\n",
    "            day_merged.append(merged_on_common)\n",
    "            day_times_start.append(t_start)\n",
    "            day_times_end.append(t_end)\n",
    "            day_incloud.append(inc_flag)\n",
    "            day_n_fit.append(opt_res[\"n_fit\"])\n",
    "            day_n_pops_fit.append(opt_res[\"n_pops_fit\"])\n",
    "            day_rho_fit.append(opt_res[\"rho_fit\"])\n",
    "            day_best_cost.append(opt_res[\"best_cost\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            err_ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "            with ERROR_LOG.open(\"a\") as ef:\n",
    "                ef.write(f\"[{err_ts}] ERROR on date {a_date} chunk {i:03d}\\n\")\n",
    "                ef.write(f\"t_start={t_start}, t_end={t_end}\\n\")\n",
    "                ef.write(f\"{type(e).__name__}: {e}\\n\")\n",
    "                ef.write(traceback.format_exc())\n",
    "                ef.write(\"\\n\")\n",
    "            with LOG_FILE.open(\"a\") as f:\n",
    "                f.write(f\"\\t[ERROR] chunk {i:03d} failed, see error_log.txt\\n\")\n",
    "\n",
    "    # write per-day (edges identical across ALL days)\n",
    "    if len(day_merged) > 0:\n",
    "        write_day_netcdf_v2(\n",
    "            day_dir,\n",
    "            a_date,\n",
    "            day_fine_edges=np.asarray(common_edges, float),  # GLOBAL_EDGES\n",
    "            day_fims_algn=np.asarray(day_fims_algn, float),\n",
    "            day_uhsas_algn=np.asarray(day_uhsas_algn, float),\n",
    "            day_pops_algn=np.asarray(day_pops_algn, float),\n",
    "            day_aps_algn=np.asarray(day_aps_algn, float),\n",
    "            day_fine_vals=np.asarray(day_merged, float),\n",
    "            day_times_start=day_times_start,\n",
    "            day_times_end=day_times_end,\n",
    "            day_incloud_flag=np.asarray(day_incloud, int),\n",
    "            day_n_fit=np.asarray(day_n_fit, float),\n",
    "            day_n_pops_fit=np.asarray(day_n_pops_fit, float),\n",
    "            day_rho_fit=np.asarray(day_rho_fit, float),\n",
    "            day_best_cost=np.asarray(day_best_cost, float),\n",
    "            orig_APS_edges=np.asarray(orig_APS_edges, float),\n",
    "            orig_UHSAS_edges=np.asarray(orig_UHSAS_edges, float),\n",
    "            orig_POPS_edges=np.asarray(orig_POPS_edges, float),\n",
    "            orig_FIMS_edges=np.asarray(orig_FIMS_edges, float),\n",
    "        )\n",
    "    else:\n",
    "        with LOG_FILE.open(\"a\") as f:\n",
    "            f.write(f\"[WARN] {a_date}: no valid chunks to write\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750dec71",
   "metadata": {},
   "source": [
    "## Current ARCSIX Campaign Size Distribution Merge R1 QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bba2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "QC plots for ARCSIX size-distribution merge output NetCDF files (NO flag filtering)\n",
    "+ write QC-flagged NetCDF copies with added warning flags.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# PATH SETTINGS\n",
    "# -------------------------------------------------------------------\n",
    "BASE_DIR = Path(\"/Users/C832577250/Output/arcsix_sizedist_merge_batch_v3\")\n",
    "\n",
    "DATA_DIR = Path(\"/Volumes/Hailstone Data/Research Data/ARCSIX_P3B\")\n",
    "micro_dir = DATA_DIR / \"LARGE-MICROPHYSICAL\"\n",
    "\n",
    "QC_DIR = BASE_DIR / \"qc_plots\"\n",
    "QC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "QC_NC_DIR = BASE_DIR / \"qc_flagged_nc\"\n",
    "QC_NC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPO_BASE = Path.cwd().parent\n",
    "sys.path.append(str(REPO_BASE / \"src\"))\n",
    "from ict_utils import read_microphysical  # noqa: E402\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# CONSTANTS / THRESHOLDS\n",
    "# -------------------------------------------------------------------\n",
    "HIGH_COST_THRESH = 0.2\n",
    "\n",
    "# cutoff for totals comparison (nm)\n",
    "DP_CUTOFF_NM = 10.0\n",
    "\n",
    "# robust outlier K for WARNING flag\n",
    "K_SIGMA_WARN = 10.0\n",
    "\n",
    "# robust outlier K for DELETING extreme chunks in qc_flagged_nc output\n",
    "K_SIGMA_DROP = 20.0\n",
    "\n",
    "MIN_POINTS_FOR_ROBUST = 50  # hard fail if too few points to define bounds\n",
    "\n",
    "# scatter plot axes\n",
    "PLOT_LO = 0.0\n",
    "PLOT_HI = 8000.0\n",
    "\n",
    "# threshold line styling (for cost hist only)\n",
    "THRESH_COLOR = \"orange\"\n",
    "THRESH_LW = 2.0\n",
    "THRESH_LS = \"-\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# SMALL HELPERS\n",
    "# -------------------------------------------------------------------\n",
    "def _read_var_as_array(ds, name: str) -> np.ndarray:\n",
    "    \"\"\"Read a variable from a Dataset and convert _FillValue to NaN.\"\"\"\n",
    "    var = ds.variables[name]\n",
    "    arr = np.array(var[:], dtype=float)\n",
    "\n",
    "    fv = getattr(var, \"_FillValue\", None)\n",
    "    if fv is not None:\n",
    "        arr[arr == fv] = np.nan\n",
    "    arr[~np.isfinite(arr)] = np.nan\n",
    "    return arr\n",
    "\n",
    "\n",
    "def _parse_base_time(ds: Dataset) -> datetime:\n",
    "    base_iso = ds.getncattr(\"base_time_iso\")\n",
    "    if base_iso.endswith(\"Z\"):\n",
    "        base_iso = base_iso[:-1]\n",
    "    if base_iso.endswith(\"+00:00\") or base_iso.endswith(\"-00:00\"):\n",
    "        base_iso = base_iso[:-6]\n",
    "    base_dt = datetime.fromisoformat(base_iso)\n",
    "    if base_dt.tzinfo is not None:\n",
    "        base_dt = base_dt.replace(tzinfo=None)\n",
    "    return base_dt\n",
    "\n",
    "\n",
    "def _ensure_naive_datetime_index(idx: pd.Index) -> pd.DatetimeIndex:\n",
    "    if not isinstance(idx, pd.DatetimeIndex):\n",
    "        raise TypeError(\"Expected a DatetimeIndex for CPC time axis.\")\n",
    "    if idx.tz is not None:\n",
    "        idx = idx.tz_convert(\"UTC\").tz_localize(None)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _add_flag_var(dst_ds: Dataset, name: str, dims: tuple, data: np.ndarray, long_name: str, comment: str):\n",
    "    \"\"\"Add int8 flag variable with 0/1.\"\"\"\n",
    "    if name in dst_ds.variables:\n",
    "        raise RuntimeError(f\"Destination file already has variable {name} (refusing to overwrite).\")\n",
    "\n",
    "    v = dst_ds.createVariable(name, \"i1\", dims)\n",
    "    v[:] = data.astype(np.int8)\n",
    "    v.setncattr(\"long_name\", long_name)\n",
    "    v.setncattr(\"units\", \"1\")\n",
    "    v.setncattr(\"flag_values\", np.array([0, 1], dtype=np.int8))\n",
    "    v.setncattr(\"flag_meanings\", \"ok warning\")\n",
    "    v.setncattr(\"comment\", comment)\n",
    "\n",
    "\n",
    "def _copy_netcdf_subset_by_dim(\n",
    "    src: Dataset,\n",
    "    dst_path: Path,\n",
    "    *,\n",
    "    subset_dim: str,\n",
    "    keep_idx: np.ndarray,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create dst NETCDF4 file and copy everything from src, but SUBSET along subset_dim for any variable\n",
    "    that contains that dimension. All other dimensions/variables copied as-is.\n",
    "\n",
    "    Returns: dst Dataset (open, caller must close).\n",
    "    \"\"\"\n",
    "    if dst_path.exists():\n",
    "        dst_path.unlink()\n",
    "\n",
    "    dst = Dataset(dst_path, mode=\"w\", format=\"NETCDF4\")\n",
    "\n",
    "    # global attrs\n",
    "    for attr in src.ncattrs():\n",
    "        dst.setncattr(attr, src.getncattr(attr))\n",
    "\n",
    "    # dims\n",
    "    keep_n = int(keep_idx.size)\n",
    "    for dname, dim in src.dimensions.items():\n",
    "        if dname == subset_dim:\n",
    "            dst.createDimension(dname, keep_n)\n",
    "        else:\n",
    "            dst.createDimension(dname, (len(dim) if not dim.isunlimited() else None))\n",
    "\n",
    "    # vars\n",
    "    for vname, svar in src.variables.items():\n",
    "        fill_value = getattr(svar, \"_FillValue\", None)\n",
    "        if fill_value is not None:\n",
    "            dvar = dst.createVariable(vname, svar.dtype, svar.dimensions, fill_value=fill_value)\n",
    "        else:\n",
    "            dvar = dst.createVariable(vname, svar.dtype, svar.dimensions)\n",
    "\n",
    "        for attr in svar.ncattrs():\n",
    "            if attr == \"_FillValue\":\n",
    "                continue\n",
    "            dvar.setncattr(attr, svar.getncattr(attr))\n",
    "\n",
    "        data = svar[:]\n",
    "        if subset_dim in svar.dimensions:\n",
    "            axis = svar.dimensions.index(subset_dim)\n",
    "            data = np.take(data, keep_idx, axis=axis)\n",
    "        dvar[:] = data\n",
    "\n",
    "    return dst\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# INTEGRATION FOR >DP_CUTOFF_NM\n",
    "# -------------------------------------------------------------------\n",
    "def _gt_cutoff_weights_from_edges(fine_edges_nm: np.ndarray, cutoff_nm: float) -> np.ndarray:\n",
    "    edges = np.asarray(fine_edges_nm, dtype=float)\n",
    "    if edges.ndim != 1 or edges.size < 2:\n",
    "        raise ValueError(\"fine_edges_nm must be a 1D array with length >= 2.\")\n",
    "    if not np.all(np.isfinite(edges)):\n",
    "        raise ValueError(\"fine_edges_nm contains non-finite values.\")\n",
    "    if not np.all(np.diff(edges) > 0):\n",
    "        raise ValueError(\"fine_edges_nm must be strictly increasing.\")\n",
    "    if not np.isfinite(cutoff_nm) or cutoff_nm <= 0:\n",
    "        raise ValueError(\"cutoff_nm must be a positive finite number.\")\n",
    "\n",
    "    nb = edges.size - 1\n",
    "    w = np.zeros(nb, dtype=float)\n",
    "\n",
    "    if cutoff_nm <= edges[0]:\n",
    "        w[:] = 1.0\n",
    "        return w\n",
    "    if cutoff_nm >= edges[-1]:\n",
    "        return w\n",
    "\n",
    "    loge = np.log10(edges)\n",
    "    i = np.searchsorted(edges, cutoff_nm, side=\"right\") - 1\n",
    "    if i < 0:\n",
    "        w[:] = 1.0\n",
    "        return w\n",
    "    if i >= nb:\n",
    "        return w\n",
    "\n",
    "    if i + 1 < nb:\n",
    "        w[i + 1:] = 1.0\n",
    "\n",
    "    lo = loge[i]\n",
    "    hi = loge[i + 1]\n",
    "    lc = np.log10(cutoff_nm)\n",
    "    frac = (hi - lc) / (hi - lo)\n",
    "    w[i] = float(np.clip(frac, 0.0, 1.0))\n",
    "    return w\n",
    "\n",
    "\n",
    "def integrate_dNdlogDp_gt_cutoff(dNdlogDp: np.ndarray, fine_edges_nm: np.ndarray, cutoff_nm: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Integrate dN/dlog10Dp over Dp>cutoff:\n",
    "      N_gt = Σ dNdlogDp * dlog10Dp * weight\n",
    "    \"\"\"\n",
    "    A = np.asarray(dNdlogDp, dtype=float)\n",
    "    if A.ndim != 2:\n",
    "        raise ValueError(\"dNdlogDp must be 2D (chunk, fine_bin).\")\n",
    "\n",
    "    edges = np.asarray(fine_edges_nm, dtype=float)\n",
    "    dlog10 = np.diff(np.log10(edges))\n",
    "    if A.shape[1] != dlog10.size:\n",
    "        raise RuntimeError(\n",
    "            f\"Shape mismatch: dNdlogDp has {A.shape[1]} bins but fine_edges_nm implies {dlog10.size} bins.\"\n",
    "        )\n",
    "\n",
    "    w = _gt_cutoff_weights_from_edges(edges, cutoff_nm)\n",
    "    contrib = A * (dlog10[None, :] * w[None, :])\n",
    "    return np.nansum(contrib, axis=1)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# ROBUST BOUNDS (LINEAR RESIDUAL metric)\n",
    "# -------------------------------------------------------------------\n",
    "def compute_robust_linear_bounds(\n",
    "    merged: np.ndarray,\n",
    "    cpc: np.ndarray,\n",
    "    *,\n",
    "    k_sigma: float,\n",
    "    min_points: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    r = (merged - cpc)\n",
    "    bounds: median(r) ± k_sigma*(1.4826*MAD(r))\n",
    "    \"\"\"\n",
    "    y = np.asarray(merged, float)\n",
    "    x = np.asarray(cpc, float)\n",
    "\n",
    "    ok = np.isfinite(x) & np.isfinite(y)\n",
    "    if not np.any(ok):\n",
    "        raise RuntimeError(\"No valid merged/cpc points for linear bounds.\")\n",
    "\n",
    "    r = (y[ok] - x[ok])\n",
    "\n",
    "    if r.size < int(min_points):\n",
    "        raise RuntimeError(f\"Not enough valid points for robust bounds: {r.size} < {min_points}\")\n",
    "\n",
    "    r_med = float(np.median(r))\n",
    "    mad = float(np.median(np.abs(r - r_med)))\n",
    "    sigma = 1.4826 * mad\n",
    "    if not np.isfinite(sigma) or sigma <= 0:\n",
    "        raise RuntimeError(f\"Invalid robust sigma: sigma={sigma} (mad={mad})\")\n",
    "\n",
    "    r_low = r_med - k_sigma * sigma\n",
    "    r_high = r_med + k_sigma * sigma\n",
    "    return float(r_low), float(r_high), r_med, float(sigma)\n",
    "\n",
    "\n",
    "def linear_resid_and_flag(\n",
    "    merged: np.ndarray,\n",
    "    cpc: np.ndarray,\n",
    "    *,\n",
    "    r_low: float,\n",
    "    r_high: float,\n",
    "):\n",
    "    \"\"\"\n",
    "    r = merged - cpc\n",
    "    flag if r outside [r_low, r_high]\n",
    "    \"\"\"\n",
    "    y = np.asarray(merged, float)\n",
    "    x = np.asarray(cpc, float)\n",
    "\n",
    "    r_full = np.full_like(x, np.nan, dtype=float)\n",
    "    ok = np.isfinite(x) & np.isfinite(y)\n",
    "    r_full[ok] = y[ok] - x[ok]\n",
    "\n",
    "    flag = ok & ((r_full < r_low) | (r_full > r_high))\n",
    "    return r_full, flag\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# GATHER EVERYTHING IN ONE PASS (FOR PLOTS/CSV/BOUNDS)\n",
    "# -------------------------------------------------------------------\n",
    "def gather_all_chunks(base_dir: Path):\n",
    "    nc_files = sorted(base_dir.glob(\"**/*_sizedist_merged_v2.nc\"))\n",
    "    if not nc_files:\n",
    "        raise FileNotFoundError(f\"No *_sizedist_merged_v2.nc files found under {base_dir}\")\n",
    "\n",
    "    print(f\"Found {len(nc_files)} NetCDF files.\")\n",
    "\n",
    "    all_cost = []\n",
    "    all_uhsas_n = []\n",
    "    all_pops_n = []\n",
    "    all_rho = []\n",
    "\n",
    "    all_cpc_median = []\n",
    "    all_merged_total_gt10 = []\n",
    "\n",
    "    micro_cache = {}\n",
    "\n",
    "    for nc_path in nc_files:\n",
    "        date_str = nc_path.stem.split(\"_\")[0]\n",
    "        print(f\"\\nProcessing {nc_path} (date {date_str})\")\n",
    "\n",
    "        if date_str not in micro_cache:\n",
    "            micro = read_microphysical(micro_dir, start=date_str, end=None, prefix=\"ARCSIX\")\n",
    "            cpc_series = pd.to_numeric(micro.get(\"CNgt10nm\"), errors=\"coerce\")\n",
    "            cpc_series.index = _ensure_naive_datetime_index(cpc_series.index)\n",
    "            micro_cache[date_str] = cpc_series\n",
    "        else:\n",
    "            cpc_series = micro_cache[date_str]\n",
    "\n",
    "        with Dataset(nc_path, mode=\"r\") as ds:\n",
    "            for v in (\n",
    "                \"optimization_best_cost\",\n",
    "                \"retrieved_uhsas_n_fit\",\n",
    "                \"retrieved_pops_n_fit\",\n",
    "                \"retrieved_aps_density\",\n",
    "                \"merged_dNdlogDp\",\n",
    "                \"fine_edges_nm\",\n",
    "                \"time_start_since_base_s\",\n",
    "                \"time_end_since_base_s\",\n",
    "            ):\n",
    "                if v not in ds.variables:\n",
    "                    raise RuntimeError(f\"{nc_path} missing {v}\")\n",
    "\n",
    "            cost = _read_var_as_array(ds, \"optimization_best_cost\")\n",
    "            uhsas_n = _read_var_as_array(ds, \"retrieved_uhsas_n_fit\")\n",
    "            pops_n = _read_var_as_array(ds, \"retrieved_pops_n_fit\")\n",
    "            rho = _read_var_as_array(ds, \"retrieved_aps_density\")\n",
    "\n",
    "            merged_dNdlogDp = _read_var_as_array(ds, \"merged_dNdlogDp\")\n",
    "            fine_edges_nm = _read_var_as_array(ds, \"fine_edges_nm\")\n",
    "\n",
    "            time_start_s = _read_var_as_array(ds, \"time_start_since_base_s\")\n",
    "            time_end_s = _read_var_as_array(ds, \"time_end_since_base_s\")\n",
    "\n",
    "            base_dt = _parse_base_time(ds)\n",
    "\n",
    "            n_chunks = uhsas_n.size\n",
    "            if not (pops_n.size == n_chunks and rho.size == n_chunks and cost.size == n_chunks):\n",
    "                raise RuntimeError(f\"{nc_path}: chunk length mismatch among cost/uhsas/pops/rho\")\n",
    "            if merged_dNdlogDp.shape[0] != n_chunks:\n",
    "                raise RuntimeError(f\"{nc_path}: merged_dNdlogDp chunk mismatch\")\n",
    "            if time_start_s.size != n_chunks or time_end_s.size != n_chunks:\n",
    "                raise RuntimeError(f\"{nc_path}: time_start/end chunk mismatch\")\n",
    "\n",
    "            merged_total_gt10 = integrate_dNdlogDp_gt_cutoff(merged_dNdlogDp, fine_edges_nm, DP_CUTOFF_NM)\n",
    "\n",
    "            cpc_median = np.full(n_chunks, np.nan, dtype=float)\n",
    "            for j in range(n_chunks):\n",
    "                if not (np.isfinite(time_start_s[j]) and np.isfinite(time_end_s[j])):\n",
    "                    continue\n",
    "                t0 = base_dt + timedelta(seconds=float(time_start_s[j]))\n",
    "                t1 = base_dt + timedelta(seconds=float(time_end_s[j]))\n",
    "                cpc_chunk = cpc_series.loc[t0:t1]\n",
    "                if cpc_chunk.size:\n",
    "                    cpc_median[j] = float(cpc_chunk.median())\n",
    "\n",
    "        for j in range(n_chunks):\n",
    "            all_cost.append(float(cost[j]) if np.isfinite(cost[j]) else np.nan)\n",
    "            all_uhsas_n.append(float(uhsas_n[j]) if np.isfinite(uhsas_n[j]) else np.nan)\n",
    "            all_pops_n.append(float(pops_n[j]) if np.isfinite(pops_n[j]) else np.nan)\n",
    "            all_rho.append(float(rho[j]) if np.isfinite(rho[j]) else np.nan)\n",
    "\n",
    "            all_cpc_median.append(float(cpc_median[j]) if np.isfinite(cpc_median[j]) else np.nan)\n",
    "            all_merged_total_gt10.append(float(merged_total_gt10[j]) if np.isfinite(merged_total_gt10[j]) else np.nan)\n",
    "\n",
    "    return (\n",
    "        np.asarray(all_cost, float),\n",
    "        np.asarray(all_uhsas_n, float),\n",
    "        np.asarray(all_pops_n, float),\n",
    "        np.asarray(all_rho, float),\n",
    "        np.asarray(all_cpc_median, float),\n",
    "        np.asarray(all_merged_total_gt10, float),\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# PLOTS\n",
    "# -------------------------------------------------------------------\n",
    "def plot_cost_hist(cost: np.ndarray, out_png: Path):\n",
    "    c = np.asarray(cost, float)\n",
    "    allv = c[np.isfinite(c)]\n",
    "    allv = allv[(allv >= 0.0) & (allv <= 1.0)]\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, 51)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    if allv.size:\n",
    "        plt.hist(allv, bins=bins, edgecolor=\"black\", alpha=0.7)\n",
    "\n",
    "    plt.axvline(HIGH_COST_THRESH, linestyle=THRESH_LS, linewidth=THRESH_LW, color=THRESH_COLOR)\n",
    "    plt.xlabel(\"optimization_best_cost\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"warning_high_cost: cost > {HIGH_COST_THRESH}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_simple_hist(data: np.ndarray, out_png: Path, xlabel: str, title: str, bins=60):\n",
    "    x = np.asarray(data, float)\n",
    "    v = x[np.isfinite(x)]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    if v.size:\n",
    "        plt.hist(v, bins=bins, edgecolor=\"black\", alpha=0.7)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_scatter_merged_vs_cpc_flagged_linear(\n",
    "    cpc: np.ndarray,\n",
    "    merged: np.ndarray,\n",
    "    flag: np.ndarray,\n",
    "    *,\n",
    "    r_low: float,\n",
    "    r_high: float,\n",
    "    out_png: Path,\n",
    "):\n",
    "    \"\"\"\n",
    "    Linear residual bounds:\n",
    "      y = x + r_low\n",
    "      y = x + r_high\n",
    "    \"\"\"\n",
    "    x = np.asarray(cpc, float)\n",
    "    y = np.asarray(merged, float)\n",
    "    f = np.asarray(flag, bool)\n",
    "\n",
    "    ok = np.isfinite(x) & np.isfinite(y) & (~f)\n",
    "    bad = np.isfinite(x) & np.isfinite(y) & f\n",
    "\n",
    "    plt.figure(figsize=(6.1, 5.5))\n",
    "    if np.any(ok):\n",
    "        plt.scatter(x[ok], y[ok], s=10, alpha=0.6, label=\"OK\")\n",
    "    if np.any(bad):\n",
    "        plt.scatter(x[bad], y[bad], s=28, alpha=0.9, marker=\"x\", label=\"warning_merged_gt10_diff_from_cpc=1\")\n",
    "\n",
    "    lo, hi = PLOT_LO, PLOT_HI\n",
    "    xx = np.linspace(lo, hi, 600)\n",
    "\n",
    "    plt.plot([lo, hi], [lo, hi], linestyle=\"dashed\", linewidth=1.2, color=\"k\", label=\"1:1\")\n",
    "    plt.plot(xx, xx + r_low, linestyle=\"dotted\", linewidth=1.2, color=\"k\", label=\"bound_low\")\n",
    "    plt.plot(xx, xx + r_high, linestyle=\"dotted\", linewidth=1.2, color=\"k\", label=\"bound_high\")\n",
    "\n",
    "    plt.xlabel(\"CPC10nm median (CNgt10nm) (#/cm$^3$)\")\n",
    "    plt.ylabel(f\"Merged total(>{DP_CUTOFF_NM:g} nm) (#/cm$^3$)\")\n",
    "    plt.xlim(lo, hi)\n",
    "    plt.ylim(lo, hi)\n",
    "    plt.title(f\"warning_merged_gt10_diff_from_cpc: robust linear residual outlier (K={K_SIGMA_WARN:g})\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# QC NETCDF WRITER (FILTER EXTREME CPC MISMATCH CHUNKS, THEN ADD FLAGS)\n",
    "# -------------------------------------------------------------------\n",
    "def write_qc_flagged_nc_files(\n",
    "    base_dir: Path,\n",
    "    out_dir: Path,\n",
    "    *,\n",
    "    # WARNING bounds\n",
    "    r_low_warn: float,\n",
    "    r_high_warn: float,\n",
    "    # robust center/scale (same used to derive DROP bounds)\n",
    "    r_med: float,\n",
    "    sigma_r: float,\n",
    "):\n",
    "    nc_files = sorted(base_dir.glob(\"**/*_sizedist_merged_v2.nc\"))\n",
    "    if not nc_files:\n",
    "        raise FileNotFoundError(f\"No *_sizedist_merged_v2.nc files found under {base_dir}\")\n",
    "\n",
    "    r_low_drop = r_med - K_SIGMA_DROP * sigma_r\n",
    "    r_high_drop = r_med + K_SIGMA_DROP * sigma_r\n",
    "\n",
    "    print(f\"\\nWill write filtered+flagged copies for {len(nc_files)} NetCDF files into:\\n  {out_dir}\\n\")\n",
    "    print(\n",
    "        f\"[CPC WARNING BOUNDS]  K_WARN={K_SIGMA_WARN:g}:  r in [{r_low_warn:.6g}, {r_high_warn:.6g}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[CPC DROP BOUNDS]     K_DROP={K_SIGMA_DROP:g}: r in [{r_low_drop:.6g}, {r_high_drop:.6g}]\"\n",
    "    )\n",
    "\n",
    "    micro_cache = {}\n",
    "    total_dropped = 0\n",
    "    total_kept = 0\n",
    "\n",
    "    for src_path in nc_files:\n",
    "        dst_path = out_dir / src_path.name\n",
    "        print(f\"\\n[QC-FLAG+FILTER] {src_path} -> {dst_path}\")\n",
    "\n",
    "        with Dataset(src_path, mode=\"r\") as src:\n",
    "            for v in (\n",
    "                \"optimization_best_cost\",\n",
    "                \"retrieved_uhsas_n_fit\",\n",
    "                \"merged_dNdlogDp\",\n",
    "                \"fine_edges_nm\",\n",
    "                \"time_start_since_base_s\",\n",
    "                \"time_end_since_base_s\",\n",
    "            ):\n",
    "                if v not in src.variables:\n",
    "                    raise RuntimeError(f\"{src_path} missing variable {v}\")\n",
    "\n",
    "            cost = _read_var_as_array(src, \"optimization_best_cost\")\n",
    "\n",
    "            uhsas_var = src.variables[\"retrieved_uhsas_n_fit\"]\n",
    "            dims = uhsas_var.dimensions\n",
    "            if len(dims) != 1:\n",
    "                raise RuntimeError(f\"{src_path}: expected retrieved_uhsas_n_fit to be 1D, got dims={dims}\")\n",
    "            chunk_dim = dims[0]\n",
    "            n_chunks = int(cost.size)\n",
    "\n",
    "            # warning_high_cost (on original)\n",
    "            high_cost_mask = np.isfinite(cost) & (cost > HIGH_COST_THRESH)\n",
    "\n",
    "            # merged/CPC residuals (on original)\n",
    "            merged_dNdlogDp = _read_var_as_array(src, \"merged_dNdlogDp\")\n",
    "            fine_edges_nm = _read_var_as_array(src, \"fine_edges_nm\")\n",
    "            time_start_s = _read_var_as_array(src, \"time_start_since_base_s\")\n",
    "            time_end_s = _read_var_as_array(src, \"time_end_since_base_s\")\n",
    "            base_dt = _parse_base_time(src)\n",
    "\n",
    "            if merged_dNdlogDp.shape[0] != n_chunks:\n",
    "                raise RuntimeError(f\"{src_path}: merged_dNdlogDp chunk dimension mismatch\")\n",
    "\n",
    "            merged_total_gt10 = integrate_dNdlogDp_gt_cutoff(merged_dNdlogDp, fine_edges_nm, DP_CUTOFF_NM)\n",
    "\n",
    "            date_str = src_path.stem.split(\"_\")[0]\n",
    "            if date_str not in micro_cache:\n",
    "                micro = read_microphysical(micro_dir, start=date_str, end=None, prefix=\"ARCSIX\")\n",
    "                cpc_series = pd.to_numeric(micro.get(\"CNgt10nm\"), errors=\"coerce\")\n",
    "                cpc_series.index = _ensure_naive_datetime_index(cpc_series.index)\n",
    "                micro_cache[date_str] = cpc_series\n",
    "            else:\n",
    "                cpc_series = micro_cache[date_str]\n",
    "\n",
    "            cpc_median = np.full(n_chunks, np.nan, dtype=float)\n",
    "            for j in range(n_chunks):\n",
    "                if not (np.isfinite(time_start_s[j]) and np.isfinite(time_end_s[j])):\n",
    "                    continue\n",
    "                t0 = base_dt + timedelta(seconds=float(time_start_s[j]))\n",
    "                t1 = base_dt + timedelta(seconds=float(time_end_s[j]))\n",
    "                cpc_chunk = cpc_series.loc[t0:t1]\n",
    "                if cpc_chunk.size:\n",
    "                    cpc_median[j] = float(cpc_chunk.median())\n",
    "\n",
    "            # residuals and WARNING flag (K_WARN=10) on original\n",
    "            r_full, warn_flag = linear_resid_and_flag(\n",
    "                merged_total_gt10,\n",
    "                cpc_median,\n",
    "                r_low=r_low_warn,\n",
    "                r_high=r_high_warn,\n",
    "            )\n",
    "\n",
    "            # DROP mask (K_DROP=20) on original: only where residual is finite\n",
    "            ok = np.isfinite(r_full)\n",
    "            drop_mask = ok & ((r_full < r_med - K_SIGMA_DROP * sigma_r) | (r_full > r_med + K_SIGMA_DROP * sigma_r))\n",
    "\n",
    "            keep_mask = ~drop_mask\n",
    "            keep_idx = np.flatnonzero(keep_mask).astype(int)\n",
    "\n",
    "            n_drop = int(np.sum(drop_mask))\n",
    "            n_keep = int(keep_idx.size)\n",
    "            total_dropped += n_drop\n",
    "            total_kept += n_keep\n",
    "\n",
    "            print(f\"    chunks: total={n_chunks}  drop_extreme(K={K_SIGMA_DROP:g})={n_drop}  keep={n_keep}\")\n",
    "            if n_keep <= 0:\n",
    "                raise RuntimeError(f\"{src_path}: all chunks were dropped by extreme CPC filter; refusing to write empty file.\")\n",
    "\n",
    "            # create subsetted copy\n",
    "            dst = _copy_netcdf_subset_by_dim(src, dst_path, subset_dim=chunk_dim, keep_idx=keep_idx)\n",
    "\n",
    "            # subset flags to kept\n",
    "            high_cost_keep = high_cost_mask[keep_idx]\n",
    "            warn_keep = warn_flag[keep_idx]\n",
    "\n",
    "            # add ONLY kept flags to dst\n",
    "            _add_flag_var(\n",
    "                dst,\n",
    "                \"warning_high_cost\",\n",
    "                (chunk_dim,),\n",
    "                high_cost_keep.astype(np.int8),\n",
    "                long_name=\"QC warning: optimization cost exceeds threshold\",\n",
    "                comment=f\"1 if optimization_best_cost > {HIGH_COST_THRESH}, else 0\",\n",
    "            )\n",
    "            _add_flag_var(\n",
    "                dst,\n",
    "                \"warning_merged_gt10_diff_from_cpc\",\n",
    "                (chunk_dim,),\n",
    "                warn_keep.astype(np.int8),\n",
    "                long_name=f\"QC warning: merged total(>{DP_CUTOFF_NM:g} nm) vs CPC is an outlier (linear residual)\",\n",
    "                comment=(\n",
    "                    f\"r = merged_total_gt{DP_CUTOFF_NM:g}nm - CPC_median. \"\n",
    "                    f\"Robust bounds: r within median±{K_SIGMA_WARN:g}*(1.4826*MAD(r)). \"\n",
    "                    f\"Computed over ALL chunks: r_med={r_med}, sigma_r={sigma_r}, \"\n",
    "                    f\"r_low_warn={r_low_warn}, r_high_warn={r_high_warn}. \"\n",
    "                    f\"Flag=1 if r outside [r_low_warn,r_high_warn]. \"\n",
    "                    f\"NOTE: extreme chunks were removed using K_DROP={K_SIGMA_DROP:g} before writing this file.\"\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"    warnings (on kept): high_cost={int(np.nansum(high_cost_keep))}, \"\n",
    "                f\"merged_linear_outlier={int(np.nansum(warn_keep))}\"\n",
    "            )\n",
    "\n",
    "            dst.close()\n",
    "\n",
    "    print(f\"\\nDone writing QC-flagged NetCDF copies.\")\n",
    "    print(f\"[TOTAL] dropped_extreme={total_dropped}  kept={total_kept}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# SUMMARY STATS (only kept warnings; computed over ALL chunks)\n",
    "# -------------------------------------------------------------------\n",
    "def print_warning_summary(flag_dict: dict[str, np.ndarray]):\n",
    "    keys = list(flag_dict.keys())\n",
    "    if not keys:\n",
    "        raise ValueError(\"flag_dict is empty\")\n",
    "\n",
    "    n = int(flag_dict[keys[0]].size)\n",
    "    for k in keys[1:]:\n",
    "        if int(flag_dict[k].size) != n:\n",
    "            raise RuntimeError(\"Flag masks have inconsistent lengths\")\n",
    "\n",
    "    any_warn = np.zeros(n, dtype=bool)\n",
    "    print(\"\\n=== WARNING SUMMARY (per-chunk; ALL chunks, K_WARN applied) ===\")\n",
    "    print(f\"Total chunks: {n}\")\n",
    "\n",
    "    for k in keys:\n",
    "        m = np.asarray(flag_dict[k], bool)\n",
    "        c = int(np.nansum(m))\n",
    "        any_warn |= m\n",
    "        pct = (100.0 * c / n) if n > 0 else np.nan\n",
    "        print(f\"  {k}: {c} ({pct:.2f}%)\")\n",
    "\n",
    "    c_any = int(np.nansum(any_warn))\n",
    "    c_free = int(n - c_any)\n",
    "    pct_any = (100.0 * c_any / n) if n > 0 else np.nan\n",
    "    pct_free = (100.0 * c_free / n) if n > 0 else np.nan\n",
    "    print(f\"  any_warning: {c_any} ({pct_any:.2f}%)\")\n",
    "    print(f\"  warning_free: {c_free} ({pct_free:.2f}%)\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# MAIN\n",
    "# -------------------------------------------------------------------\n",
    "(\n",
    "    all_cost,\n",
    "    all_uhsas_n,\n",
    "    all_pops_n,\n",
    "    all_rho,\n",
    "    all_cpc_median,\n",
    "    all_merged_total_gt10,\n",
    ") = gather_all_chunks(BASE_DIR)\n",
    "\n",
    "# warning_high_cost mask\n",
    "mask_cost = np.isfinite(all_cost) & (all_cost > HIGH_COST_THRESH)\n",
    "\n",
    "# robust bounds for merged/CPC using LINEAR residual (WARNING K=10)\n",
    "r_low_warn, r_high_warn, r_med, sigma_r = compute_robust_linear_bounds(\n",
    "    merged=all_merged_total_gt10,\n",
    "    cpc=all_cpc_median,\n",
    "    k_sigma=K_SIGMA_WARN,\n",
    "    min_points=MIN_POINTS_FOR_ROBUST,\n",
    ")\n",
    "\n",
    "r_metric, mask_merged_vs_cpc_warn = linear_resid_and_flag(\n",
    "    all_merged_total_gt10,\n",
    "    all_cpc_median,\n",
    "    r_low=r_low_warn,\n",
    "    r_high=r_high_warn,\n",
    ")\n",
    "\n",
    "# drop bounds (for info)\n",
    "r_low_drop = r_med - K_SIGMA_DROP * sigma_r\n",
    "r_high_drop = r_med + K_SIGMA_DROP * sigma_r\n",
    "\n",
    "print(\n",
    "    f\"\\n[MERGED/CPC LINEAR] r_med={r_med:.6g}, sigma_r={sigma_r:.6g} (sigma=1.4826*MAD), \"\n",
    "    f\"K_WARN={K_SIGMA_WARN:g}, K_DROP={K_SIGMA_DROP:g}\"\n",
    ")\n",
    "print(\n",
    "    f\"[WARNING BOUNDS] r_low_warn={r_low_warn:.6g}, r_high_warn={r_high_warn:.6g}\"\n",
    ")\n",
    "print(\n",
    "    f\"[DROP BOUNDS]    r_low_drop={r_low_drop:.6g}, r_high_drop={r_high_drop:.6g}\"\n",
    ")\n",
    "\n",
    "valid = np.isfinite(r_metric)\n",
    "print(f\"[MERGED/CPC WARNING FLAG COUNT] {int(np.nansum(mask_merged_vs_cpc_warn))} flagged / {int(np.sum(valid))} valid\")\n",
    "\n",
    "# PLOTS\n",
    "plot_cost_hist(all_cost, QC_DIR / \"hist_optimization_best_cost_with_thresh.png\")\n",
    "\n",
    "# Keep histograms but NO threshold lines\n",
    "plot_simple_hist(all_uhsas_n, QC_DIR / \"hist_uhsas_n.png\", xlabel=\"retrieved_uhsas_n_fit\", title=\"UHSAS n distribution\")\n",
    "plot_simple_hist(all_pops_n, QC_DIR / \"hist_pops_n.png\", xlabel=\"retrieved_pops_n_fit\", title=\"POPS n distribution\")\n",
    "plot_simple_hist(all_rho, QC_DIR / \"hist_aps_rho.png\", xlabel=\"retrieved_aps_density (kg m$^{-3}$)\", title=\"APS density distribution\")\n",
    "\n",
    "plot_scatter_merged_vs_cpc_flagged_linear(\n",
    "    cpc=all_cpc_median,\n",
    "    merged=all_merged_total_gt10,\n",
    "    flag=mask_merged_vs_cpc_warn,\n",
    "    r_low=r_low_warn,\n",
    "    r_high=r_high_warn,\n",
    "    out_png=QC_DIR / \"scatter_merged_gt10nm_vs_cpc_flagged.png\",\n",
    ")\n",
    "\n",
    "# SUMMARY STATS (only kept warnings; WARNING masks with K_WARN)\n",
    "print_warning_summary(\n",
    "    {\n",
    "        \"warning_high_cost\": mask_cost,\n",
    "        \"warning_merged_gt10_diff_from_cpc\": mask_merged_vs_cpc_warn,\n",
    "    }\n",
    ")\n",
    "\n",
    "# CSV (keep distributions + WARNING flags)\n",
    "out_csv = QC_DIR / \"per_chunk_qc_with_flags.csv\"\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"optimization_best_cost\": all_cost,\n",
    "        \"retrieved_uhsas_n_fit\": all_uhsas_n,\n",
    "        \"retrieved_pops_n_fit\": all_pops_n,\n",
    "        \"retrieved_aps_density\": all_rho,\n",
    "        \"CPC_median_CNgt10nm\": all_cpc_median,\n",
    "        f\"MERGED_total_gt{int(DP_CUTOFF_NM)}nm\": all_merged_total_gt10,\n",
    "        \"linear_residual_merged_minus_cpc\": r_metric,\n",
    "        \"warning_high_cost\": mask_cost.astype(int),\n",
    "        \"warning_merged_gt10_diff_from_cpc\": mask_merged_vs_cpc_warn.astype(int),\n",
    "    }\n",
    ")\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(f\"\\nWrote per-chunk QC table to {out_csv}\")\n",
    "\n",
    "# Write QC-flagged NetCDF copies:\n",
    "# - delete extreme CPC mismatch chunks using K_DROP=20\n",
    "# - then add warning_high_cost + warning_merged_gt10_diff_from_cpc (K_WARN=10) on remaining chunks\n",
    "write_qc_flagged_nc_files(\n",
    "    BASE_DIR,\n",
    "    QC_NC_DIR,\n",
    "    r_low_warn=r_low_warn,\n",
    "    r_high_warn=r_high_warn,\n",
    "    r_med=r_med,\n",
    "    sigma_r=sigma_r,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
